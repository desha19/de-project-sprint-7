{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5634c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"YARN_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223a1208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:23:00 WARN Utils: Your hostname, fhmugce59tbv43vsp0ua resolves to a loopback address: 127.0.1.1; using 172.16.0.3 instead (on interface eth0)\n",
      "24/09/03 13:23:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/03 13:23:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/03 13:23:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .appName(\"Project_7_4\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52a4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_transform(geo_path: str, sql) -> DataFrame:\n",
    "    cities_geo = (sql.read.option(\"header\", True)\n",
    "            .option(\"delimiter\", \";\")\n",
    "            .csv(geo_path)\n",
    "            .withColumn(\"lat_g\", F.regexp_replace(\"lat\", \",\", \".\").cast(\"float\"))\n",
    "            .withColumn(\"lng_g\", F.regexp_replace(\"lng\", \",\", \".\").cast('float'))\n",
    "            .drop(\"lat\", \"lng\")\n",
    "            .persist()\n",
    "            )\n",
    "    return cities_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ad8876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+--------+\n",
      "| id|      city|   lat_g|   lng_g|\n",
      "+---+----------+--------+--------+\n",
      "|  1|    Sydney| -33.865|151.2094|\n",
      "|  2| Melbourne|-37.8136|144.9631|\n",
      "|  3|  Brisbane|-27.4678|153.0281|\n",
      "|  4|     Perth|-31.9522|115.8589|\n",
      "|  5|  Adelaide|-34.9289|138.6011|\n",
      "|  6|Gold Coast|-28.0167|   153.4|\n",
      "|  7|Cranbourne|-38.0996|145.2834|\n",
      "|  8|  Canberra|-35.2931|149.1269|\n",
      "|  9| Newcastle|-32.9167|  151.75|\n",
      "| 10|Wollongong|-34.4331|150.8831|\n",
      "| 11|   Geelong|  -38.15|  144.35|\n",
      "| 12|    Hobart|-42.8806| 147.325|\n",
      "| 13|Townsville|-19.2564|146.8183|\n",
      "| 14|   Ipswich|-27.6167|152.7667|\n",
      "| 15|    Cairns|-16.9303|145.7703|\n",
      "| 16| Toowoomba|-27.5667|  151.95|\n",
      "| 17|    Darwin|-12.4381|130.8411|\n",
      "| 18|  Ballarat|  -37.55|  143.85|\n",
      "| 19|   Bendigo|  -36.75|144.2667|\n",
      "| 20|Launceston|-41.4419| 147.145|\n",
      "+---+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "geo_transform_df = geo_transform(\"/user/denis19/data/geo/cities/actual/geo.csv\", spark)\n",
    "geo_transform_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d06512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_transform_from(events_path: str, sql) -> DataFrame:\n",
    "    events_transform_from = (sql\n",
    "        .read.parquet(events_path)\n",
    "        # отобрать только те строки, где \"event_type\" = \"message\"\n",
    "        .where('event_type = \"message\"')\n",
    "        # отбираем необходимые столбцы\n",
    "        .selectExpr(\"event.message_id as message_id_from\", \"event.message_from\", \"event.subscription_channel\", \"lat\", \"lon\", \"date\")\n",
    "        # отбираем только те строки, где нет NULL значений\n",
    "        .where(\"lat IS NOT NULL and lon IS NOT NULL\")\n",
    "        # переименовываем столбцы для удобства\n",
    "        .withColumnRenamed(\"lat\", \"lat_eff\")  \n",
    "        .withColumnRenamed(\"lon\", \"lon_eff\")\n",
    "        # отбираем только те строки, где нет NULL значений  \n",
    "        .where(\"message_from IS NOT NULL\")\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    window = Window().partitionBy(\"message_from\").orderBy(F.col(\"date\").desc())\n",
    "    events_transform_from = (\n",
    "        events_transform_from\n",
    "        # добавляем новый столбец \"row_number\", который содержит номер строки в каждой группе, отсортированной по дате\n",
    "        .withColumn(\"row_number\", F.row_number().over(window))\n",
    "        # фильтруем строки, оставляя только первую строку в каждой группе (самую последнюю по дате)\n",
    "        .filter(F.col(\"row_number\") == 1)\n",
    "        .drop(\"row_number\")\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    return events_transform_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3baad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/03 13:23:52 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+-------+-------+----+\n",
      "|message_id_from|message_from|subscription_channel|lat_eff|lon_eff|date|\n",
      "+---------------+------------+--------------------+-------+-------+----+\n",
      "+---------------+------------+--------------------+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "events_transform_from_df = events_transform_from(\"/user/master/data/geo/events\", spark)\n",
    "events_transform_from_df.filter(F.col(\"subscription_channel\").isNotNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea232410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_subscriptions(events_path: str, sql) -> DataFrame:\n",
    "    # чтение паркет файла и переименовываем столбц \"subscription_channel\" на \"ch\"\n",
    "    events_subscription = (sql\n",
    "        .read.parquet(events_path)\n",
    "        .selectExpr(\"event.user as user\", \"event.subscription_channel as ch\") \n",
    "        .where(\"user is not null and ch is not null\")\n",
    "        # группируем строки по столбцу \"user\". Для каждой группы собираем все значения \"ch\" в список и сохраняем столбец \"chans\"\n",
    "        .groupBy(\"user\").agg(F.collect_list(F.col(\"ch\")).alias(\"chans\"))\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    return events_subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7c827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>(197 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  user|               chans|\n",
      "+------+--------------------+\n",
      "|100010|[115991, 921975, ...|\n",
      "|100140|[181194, 272628, ...|\n",
      "|100227|[480141, 371174, ...|\n",
      "|100263|[10793, 337169, 3...|\n",
      "|100320|[420819, 72394, 1...|\n",
      "|100553|[745258, 918447, ...|\n",
      "|100704|[278782, 734173, ...|\n",
      "|100735|[416809, 791826, ...|\n",
      "|100768|[291826, 559591, ...|\n",
      "| 10096|[322875, 777239, ...|\n",
      "|100964|[77250, 995213, 1...|\n",
      "|101021|[215678, 225805, ...|\n",
      "|101122|[695155, 293747, ...|\n",
      "|101205|[317690, 422436, ...|\n",
      "|101261|[891635, 170338, ...|\n",
      "|101272|[534280, 486852, ...|\n",
      "|102113|[411153, 724092, ...|\n",
      "|102521|[81758, 111882, 6...|\n",
      "|102536|[141196, 136143, ...|\n",
      "|102539|[310577, 827713, ...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:=====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "events_subscriptions_df = events_subscriptions(\"/user/master/data/geo/events\", spark)\n",
    "events_subscriptions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ed0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_union_sender_receiver(events_path: str, sql) -> DataFrame:\n",
    "    # производим чтение паркет файла выбираеи и переименовываем отобранные столбцы \"message_from\" на \"sender\", \"message_to\" на \"reciever\"\n",
    "    sender_receiver_df = (sql\n",
    "        .read.parquet(events_path)\n",
    "        .selectExpr(\"event.message_from as sender\", \"event.message_to as reciever\") \n",
    "        .where(\"sender is not null and reciever is not null\")\n",
    "    )\n",
    "    # производим чтение паркет файла выбираеи и переименовываем отобранные столбцы \"message_from\" на \"sender\", \"message_to\" на \"reciever\"\n",
    "    receiver_sender_df = (sql\n",
    "        .read.parquet(events_path)\n",
    "        .selectExpr(\"event.message_to as reciever\", \"event.message_from as sender\") \n",
    "        .where(\"sender is not null and reciever is not null\")\n",
    "    )\n",
    "    # проводим объединение \"sender_receiver_df\" и \"receiver_sender_df\" и удаляем дубликаты \n",
    "    events_union_sender_receiver_df = (sender_receiver_df\n",
    "        .union(receiver_sender_df)\n",
    "        .distinct()\n",
    "    )\n",
    "    # добавляем новый столбец \"sender_reciever_existing\", который содержит строку, объединяющую значения \"sender\" и \"reciever\"\n",
    "    events_union_sender_receiver_df = (events_union_sender_receiver_df\n",
    "        .withColumn(\"sender_reciever_existing\", F.concat(events_union_sender_receiver_df.sender, F.lit(\"-\"), events_union_sender_receiver_df.reciever))\n",
    "        # удаляем не нужные столбцы\n",
    "        .drop(\"sender\", \"reciever\")\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    return events_union_sender_receiver_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5189f0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:====================================================> (194 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|sender_reciever_existing|\n",
      "+------------------------+\n",
      "|           111641-149488|\n",
      "|            162624-32874|\n",
      "|            148130-37048|\n",
      "|            63964-149488|\n",
      "|             74053-94916|\n",
      "|             49528-13098|\n",
      "|              8988-83233|\n",
      "|            82108-149488|\n",
      "|             93872-81848|\n",
      "|           145291-149488|\n",
      "|            18561-149488|\n",
      "|              74682-9550|\n",
      "|            87276-149488|\n",
      "|              3175-39696|\n",
      "|           139087-107225|\n",
      "|            43551-144277|\n",
      "|           114517-149488|\n",
      "|            46022-149488|\n",
      "|             89683-71316|\n",
      "|            136390-96491|\n",
      "+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:=====================================================>(198 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "events_union_sender_receiver_df = events_union_sender_receiver(\"/user/master/data/geo/events\", spark)\n",
    "events_union_sender_receiver_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48f549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Исправление функции\n",
    "def recommendations(events_transform_from: DataFrame, geo_transform: DataFrame, events_subscription: DataFrame, events_union_sender_receiver: DataFrame) -> DataFrame:\n",
    "    result = (\n",
    "        events_transform_from.alias(\"from1\")\n",
    "            .crossJoin(events_transform_from.alias(\"from2\"))\n",
    "            .withColumn(\"distance\", F.lit(2) * F.lit(6371) * F.asin(\n",
    "                F.sqrt(\n",
    "                F.pow(F.sin((F.col('from1.lat_eff') - F.col('from2.lat_eff')) / F.lit(2)),2)\n",
    "                + F.cos(F.col(\"from1.lat_eff\"))*F.cos(F.col(\"from2.lat_eff\")) *\n",
    "                F.pow(F.sin((F.col('from1.lon_eff') - F.col('from2.lon_eff')) / F.lit(2)),2)\n",
    "        )))\n",
    "        .where(\"distance <= 1\")\n",
    "        .withColumn(\"middle_point_lat\", (F.col('from1.lat_eff') + F.col('from2.lat_eff'))/F.lit(2))\n",
    "        .withColumn(\"middle_point_lon\", (F.col('from1.lon_eff') + F.col('from2.lon_eff'))/F.lit(2))\n",
    "        .selectExpr(\"from1.message_id_from as user_left\", \"from2.message_id_from as user_right\", \"middle_point_lat\", \"middle_point_lon\")\n",
    "        .distinct()\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    result = (\n",
    "        result\n",
    "        .crossJoin(geo_transform)\n",
    "        .withColumn(\"distance\", F.lit(2) * F.lit(6371) * F.asin(\n",
    "        F.sqrt(\n",
    "            F.pow(F.sin((F.col('middle_point_lat') - F.col('lat_g'))/F.lit(2)),2)\n",
    "            + F.cos(F.col(\"middle_point_lat\"))*F.cos(F.col(\"lat_g\"))*\n",
    "            F.pow(F.sin((F.col('middle_point_lon') - F.col('lng_g'))/F.lit(2)),2)\n",
    "        )))\n",
    "        .select(\"user_left\", \"user_right\", \"id\", \"city\", \"distance\")\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    window = Window().partitionBy(\"user_left\", \"user_right\").orderBy(F.col('distance').asc())\n",
    "    result = (\n",
    "        result\n",
    "            .withColumn(\"row_number\", F.row_number().over(window))\n",
    "            .filter(F.col('row_number') == 1)\n",
    "            .drop('row_number', \"distance\", \"id\")\n",
    "            .withColumn(\"timezone\", F.concat(F.lit(\"Australia/\"), F.col(\"city\")))\n",
    "            .withColumnRenamed(\"city\", \"zone_id\")\n",
    "            .withColumn('sender_reciever_all', F.concat(result.user_left, F.lit(\"-\"), result.user_right))\n",
    "            .persist()\n",
    "    )\n",
    "    \n",
    "    result = result.join(events_union_sender_receiver, result.sender_reciever_all == events_union_sender_receiver.sender_reciever_existing, \"leftanti\")\n",
    "    result = (\n",
    "        result\n",
    "            .join(events_subscription, result.user_left == events_subscription.user, \"left\")\n",
    "            .withColumnRenamed('chans', 'chans_left')\n",
    "            .drop('user')\n",
    "            .join(events_subscription, result.user_right == events_subscription.user, \"left\")\n",
    "            .withColumnRenamed('chans', 'chans_right')\n",
    "            .drop('user')\n",
    "            .withColumn('inter_chans', F.array_intersect(F.col('chans_left'), F.col('chans_right')))\n",
    "            .filter(F.size(F.col(\"inter_chans\")) > 1)\n",
    "            .where(\"user_left <> user_right\")\n",
    "            .drop(\"inter_chans\", \"chans_left\", \"chans_right\", \"sender_reciever_all\")\n",
    "            .withColumn(\"processed_dttm\", F.current_timestamp())\n",
    "            .withColumn('local_time', \n",
    "                    F.when(F.col('zone_id')\n",
    "                           .isin('Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Canberra', 'Hobart', 'Darwin'), \n",
    "                               F.from_utc_timestamp(\n",
    "                                   F.col('processed_dttm'), \n",
    "                                                F.concat(F.lit('Australia/'), \n",
    "                                                         F.col('zone_id')))).otherwise(None))\n",
    "            .persist()\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9529f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:======>                                                (24 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "recommendations_df = recommendations(events_transform_from_df, geo_transform_df, events_subscriptions_df, events_union_sender_receiver_df)\n",
    "recommendations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    #events_path = sys.argv[1]\n",
    "    #geo_path = sys.argv[2]\n",
    "    #output_path = sys.argv[3]\n",
    "    events_path = \"/user/master/data/geo/events/\"\n",
    "    geo_path = \"/user/denis19/data/geo/cities/actual/geo.csv\"\n",
    "    output_path = \"/user/denis19/analytics/showcases/\"\n",
    "\n",
    "    conf = (SparkConf()\n",
    "        .setAppName(\"showcase_recommendations_to_friends\")\n",
    "        .set(\"spark.executor.memory\", \"4g\")\n",
    "        .set(\"spark.driver.memory\", \"4g\"))\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql = SQLContext(sc)\n",
    "\n",
    "    geo_transform_df = geo_transform(geo_path, sql)\n",
    "    events_transform_from_df = events_transform_from(events_path, sql)\n",
    "    #events_transform_to_df = events_transform_to(events_path, sql)\n",
    "    events_subscriptions_df = events_subscriptions(events_path, sql)\n",
    "    events_union_sender_receiver_df = events_union_sender_receiver(events_path, sql)\n",
    "    recommendations_df = recommendations(events_transform_from_df, geo_transform_df, events_subscriptions_df, events_union_sender_receiver_df)\n",
    "    write = recommendations_df.write.mode('overwrite').parquet(f'{output_path}')\n",
    "\n",
    "    return write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
